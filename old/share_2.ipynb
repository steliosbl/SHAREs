{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d6b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import os \n",
    "\n",
    "# Data\n",
    "from datasets import Rossi, Metabric, GBSG2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Fitness\n",
    "from lifelines.utils import concordance_index\n",
    "from gplearn.gplearn.fitness import make_fitness\n",
    "from pycox.evaluation.metrics import partial_log_likelihood_ph\n",
    "from sksurv.util import Surv\n",
    "from sksurv.metrics import integrated_brier_score\n",
    "\n",
    "# Wrapper \n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from pycox.models.cox import _CoxPHBase\n",
    "import torchtuples as tt\n",
    "\n",
    "# SHAREs\n",
    "from gplearn.gplearn.genetic import SymbolicRegressor\n",
    "from gplearn.gplearn.model import ShapeNN\n",
    "from experiments.utils import (\n",
    "    load_share_from_checkpoint,\n",
    "    get_n_shapes,\n",
    "    get_n_variables,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d448322f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Data #####\n",
    "DATASETS = dict(\n",
    "    rossi=Rossi(),\n",
    "    metabric=Metabric(),\n",
    "    gbsg2=GBSG2(),\n",
    ")\n",
    "\n",
    "\n",
    "##### Fitness Functions #####\n",
    "def metric_c_index(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Protected concordance score metric for gplearn. Greater is better.\n",
    "    \"\"\"\n",
    "    # y_true is the event time, y_pred is the predicted risk\n",
    "    # sample_weight is the event indicator\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "        try:\n",
    "            return concordance_index(y_true, np.exp(y_pred), sample_weight)\n",
    "        except ZeroDivisionError:  # In case of no unambigous pairs\n",
    "            return 0.5\n",
    "        except RuntimeWarning:  # In case of invalid log or exp overflow\n",
    "            return 0.5\n",
    "\n",
    "\n",
    "def metric_partial_likelihood(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Cox partial likelihood metric for gplearn. Less is better.\n",
    "    \"\"\"\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"error\", category=RuntimeWarning)\n",
    "        # We want to minimise the negative partial log likelihood\n",
    "        # y_pred should be the LOG partial hazards - i.e. we admit negative vaues\n",
    "        try:\n",
    "            pll = partial_log_likelihood_ph(y_pred, y_true, sample_weight, mean=True)\n",
    "            return -pll  # Flip the sign to make it a minimization function\n",
    "        except RuntimeWarning:  # In case of invalid log or exp overflow\n",
    "            return np.inf\n",
    "\n",
    "\n",
    "def metric_integrated_brier(surv_pred, E_train, T_train, E_test=None, T_test=None):\n",
    "    \"\"\"\n",
    "    Integrated Brier score for pycox-type models\n",
    "    \"\"\"\n",
    "    E_test = E_test if E_test is not None else E_train\n",
    "    T_test = T_test if T_test is not None else T_train\n",
    "\n",
    "    y_train, y_test = Surv.from_arrays(E_train, T_train), Surv.from_arrays(\n",
    "        E_test, T_test\n",
    "    )\n",
    "\n",
    "    times = surv_pred.index.values[1:-1]\n",
    "    times = times[(times > T_test.min()) & (times < T_test.max())]\n",
    "    surv_pred = surv_pred.loc[times, :].T\n",
    "\n",
    "    return integrated_brier_score(y_train, y_test, surv_pred, times)\n",
    "\n",
    "\n",
    "def fitness_c_shrink(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Concordance index with shrinkage penalty for gplearn. Greater is better.\n",
    "    \"\"\"\n",
    "    return metric_c_index(y_true, y_pred, sample_weight) - 0.05 * np.abs(y_pred).mean()\n",
    "\n",
    "\n",
    "def fitness_pll_shrink(y_true, y_pred, sample_weight):\n",
    "    \"\"\"\n",
    "    Partial log-likelihood with shrinkage penalty for gplearn. Smaller is better.\n",
    "    \"\"\"\n",
    "    pll = metric_partial_likelihood(y_true, y_pred, sample_weight)\n",
    "    return pll + 0.05 * np.abs(y_pred).mean()\n",
    "\n",
    "\n",
    "FITNESS = dict(\n",
    "    c_index=make_fitness(function=metric_c_index, greater_is_better=True),\n",
    "    c_shrink=make_fitness(function=fitness_c_shrink, greater_is_better=True),\n",
    "    pll_shrink=make_fitness(function=fitness_pll_shrink, greater_is_better=False),\n",
    ")\n",
    "\n",
    "##### Wrapper #####\n",
    "class SymRegPH(BaseEstimator, RegressorMixin, _CoxPHBase):\n",
    "    \"\"\"\n",
    "    Wrapper for gplearn's SymbolicRegressor to use with pycox supporting functions\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        if isinstance(X, tt.TupleTree):\n",
    "            X = X[0]\n",
    "        return self.model.predict(X)\n",
    "\n",
    "\n",
    "\n",
    "##### SHAREs #####\n",
    "def init_share_regressor(metric, device, checkpoint_dir, categorical_variables={}):\n",
    "    gp_config = {\n",
    "        \"population_size\": 2,\n",
    "        \"generations\": 2,\n",
    "        \"tournament_size\": 10,\n",
    "        \"function_set\": (\"add\", \"mul\", \"div\", \"shape\"),\n",
    "        \"verbose\": True,\n",
    "        \"random_state\": 42,\n",
    "        \"const_range\": None,\n",
    "        \"n_jobs\": 1,\n",
    "        \"p_crossover\": 0.4,\n",
    "        \"p_subtree_mutation\": 0.2,\n",
    "        \"p_point_mutation\": 0.2,\n",
    "        \"p_hoist_mutation\": 0.05,\n",
    "        \"p_point_replace\": 0.2,\n",
    "        \"parsimony_coefficient\": 0.0,\n",
    "        \"metric\": metric,\n",
    "        \"parsimony_coefficient\": 0.0,\n",
    "        \"optim_dict\": {\n",
    "            \"alg\": \"adam\",\n",
    "            \"lr\": 1e-2,  # tuned automatically\n",
    "            \"max_n_epochs\": 1000,\n",
    "            \"tol\": 1e-3,\n",
    "            \"task\": \"regression\",\n",
    "            \"device\": device,\n",
    "            \"batch_size\": 1000,\n",
    "            \"shape_class\": ShapeNN,\n",
    "            \"constructor_dict\": {\n",
    "                \"n_hidden_layers\": 5,\n",
    "                \"width\": 10,\n",
    "                \"activation_name\": \"ELU\",\n",
    "            },\n",
    "            \"num_workers_dataloader\": 0,\n",
    "            \"seed\": 42,\n",
    "            \"checkpoint_folder\": checkpoint_dir,\n",
    "            \"keep_models\": True,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    return SymbolicRegressor(**gp_config, categorical_variables=categorical_variables)\n",
    "\n",
    "\n",
    "def test_share_ph(\n",
    "    dataset_name, metric_name, device, checkpoint_dir, categorical_variables=False\n",
    "):\n",
    "    # Prepare dataset\n",
    "    dataset = DATASETS[dataset_name]\n",
    "    X, T, E = dataset.load(normalise=False)\n",
    "    X_train, X_test, T_train, T_test, E_train, E_test = train_test_split(\n",
    "        X, T, E, test_size=0.2, random_state=42\n",
    "    )\n",
    "    feature_names = dataset.features\n",
    "    categoricals = dataset.categorical_dict if categorical_variables else {}\n",
    "\n",
    "    # Initialise model\n",
    "    fitness = FITNESS[metric_name]\n",
    "\n",
    "    # Fit model\n",
    "    print(\"Starting model fit\")\n",
    "    # model.fit(X_train, T_train, sample_weight=E_train)\n",
    "    print(\"Finished model fit\")\n",
    "    timestamp = max(\n",
    "        os.listdir(checkpoint_dir),\n",
    "        key=lambda x: datetime.strptime(x, \"%Y-%m-%dT%H.%M.%S\"),\n",
    "    )\n",
    "\n",
    "    # Load results dataframe\n",
    "    results_df = pd.read_csv(checkpoint_dir / timestamp / \"output.csv\")\n",
    "\n",
    "    print(\"Adding validation results\")\n",
    "    n_shapes, n_variables, loss_train, loss_test, c_test, brier_test = [], [], [], [], [], []\n",
    "    for idx, id, eq, *_, in results_df.itertuples():\n",
    "        print(eq)\n",
    "        n_shapes.append(get_n_shapes(eq))\n",
    "        n_variables.append(get_n_variables(eq))\n",
    "\n",
    "        esr = load_share_from_checkpoint(\n",
    "            timestamp,\n",
    "            eq,\n",
    "            checkpoint_dir=checkpoint_dir,\n",
    "            task=\"regression\",\n",
    "            n_features=len(feature_names),\n",
    "            equation_id=id,\n",
    "        )\n",
    "\n",
    "        esr_wrap = SymRegPH(model=esr)\n",
    "        h_train, h_test = esr_wrap.predict(X_train), esr_wrap.predict(X_test)\n",
    "        h0 = esr_wrap.compute_baseline_hazards(X_train, (T_train, E_train))\n",
    "\n",
    "        loss_train.append(fitness(T_train, h_train, E_train))\n",
    "\n",
    "\n",
    "    results_df[\"loss_train\"] = loss_train\n",
    "\n",
    "    # ad-hoc correction for flipped sign in y_pred\n",
    "    if results_df[\"c_test\"].mean() < 0.5:\n",
    "        results_df[\"c_test\"] = 1 - results_df[\"c_test\"]\n",
    "\n",
    "    out_path = checkpoint_dir / timestamp / \"output2.csv\"\n",
    "    print(f\"Saving results to {out_path}\")\n",
    "    results_df.to_csv(out_path, index=False)\n",
    "\n",
    "    print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
